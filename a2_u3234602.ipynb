{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f314ad79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "  Downloading smart_open-7.0.1-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/site-packages (from gensim) (1.21.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.8/site-packages (from gensim) (1.7.1)\n",
      "Collecting wrapt\n",
      "  Downloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wrapt, smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-7.0.1 wrapt-1.16.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc8bf8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "csv.field_size_limit(1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fad41b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"state-of-the-union.csv\")\n",
    "df.columns=['year_of_the_speech','text_of_the_speech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "204ddb4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_of_the_speech</th>\n",
       "      <th>text_of_the_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1790</td>\n",
       "      <td>\\nState of the Union Address\\nGeorge Washingto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1791</td>\n",
       "      <td>\\nState of the Union Address\\nGeorge Washingto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1792</td>\n",
       "      <td>\\nState of the Union Address\\nGeorge Washingto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1793</td>\n",
       "      <td>\\nState of the Union Address\\nGeorge Washingto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1794</td>\n",
       "      <td>\\nState of the Union Address\\nGeorge Washingto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>2008</td>\n",
       "      <td>\\nState of the Union Address\\nGeorge W. Bush\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>2009</td>\n",
       "      <td>\\nAddress to Joint Session of Congress \\nBarac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>2010</td>\n",
       "      <td>\\nState of the Union Address\\nBarack Obama \\nJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>2011</td>\n",
       "      <td>\\nState of the Union Address\\nBarack Obama \\nJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2012</td>\n",
       "      <td>\\nState of the Union Address\\nBarack Obama \\nJ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year_of_the_speech                                 text_of_the_speech\n",
       "0                  1790  \\nState of the Union Address\\nGeorge Washingto...\n",
       "1                  1791  \\nState of the Union Address\\nGeorge Washingto...\n",
       "2                  1792  \\nState of the Union Address\\nGeorge Washingto...\n",
       "3                  1793  \\nState of the Union Address\\nGeorge Washingto...\n",
       "4                  1794  \\nState of the Union Address\\nGeorge Washingto...\n",
       "..                  ...                                                ...\n",
       "220                2008  \\nState of the Union Address\\nGeorge W. Bush\\n...\n",
       "221                2009  \\nAddress to Joint Session of Congress \\nBarac...\n",
       "222                2010  \\nState of the Union Address\\nBarack Obama \\nJ...\n",
       "223                2011  \\nState of the Union Address\\nBarack Obama \\nJ...\n",
       "224                2012  \\nState of the Union Address\\nBarack Obama \\nJ...\n",
       "\n",
       "[225 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a830930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df[\"text_of_the_speech\"].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a3d1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_from_git = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can't\", \"cannot\", \"could\", \"couldn't\", \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"mustn't\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"shan't\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"shouldn't\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"won't\", \"would\", \"wouldn't\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "681b66d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56ebe6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "stopwords.extend(stopwords_from_git)\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebd04b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re\n",
    "import pickle\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "def stemming(tokens):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return [stemmer.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "858f300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce2bb01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    # tokens\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t not in stopwords and t.isalpha()]\n",
    "    tokens = [t for t in tokens if len(t) > 1] # removing stopwords and punctuations\n",
    "    \n",
    "    # lammetize\n",
    "    lemmatized_tokens = lemmatize(tokens)\n",
    "    \n",
    "    # stemming\n",
    "    stemmed_tokens = stemming(lemmatized_tokens)\n",
    "    \n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08657334",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"processed_speech\"] = df.text_of_the_speech.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1153f217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_of_the_speech</th>\n",
       "      <th>text_of_the_speech</th>\n",
       "      <th>processed_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1790</td>\n",
       "      <td>\\nState of the Union Address\\nGeorge Washingto...</td>\n",
       "      <td>[state, union, address, georg, washington, dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1791</td>\n",
       "      <td>\\nState of the Union Address\\nGeorge Washingto...</td>\n",
       "      <td>[state, union, address, georg, washington, oct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1792</td>\n",
       "      <td>\\nState of the Union Address\\nGeorge Washingto...</td>\n",
       "      <td>[state, union, address, georg, washington, nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1793</td>\n",
       "      <td>\\nState of the Union Address\\nGeorge Washingto...</td>\n",
       "      <td>[state, union, address, georg, washington, dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1794</td>\n",
       "      <td>\\nState of the Union Address\\nGeorge Washingto...</td>\n",
       "      <td>[state, union, address, georg, washington, nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>2008</td>\n",
       "      <td>\\nState of the Union Address\\nGeorge W. Bush\\n...</td>\n",
       "      <td>[state, union, address, georg, bush, januari, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>2009</td>\n",
       "      <td>\\nAddress to Joint Session of Congress \\nBarac...</td>\n",
       "      <td>[address, joint, session, congress, barack, ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>2010</td>\n",
       "      <td>\\nState of the Union Address\\nBarack Obama \\nJ...</td>\n",
       "      <td>[state, union, address, barack, obama, januari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>2011</td>\n",
       "      <td>\\nState of the Union Address\\nBarack Obama \\nJ...</td>\n",
       "      <td>[state, union, address, barack, obama, januari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2012</td>\n",
       "      <td>\\nState of the Union Address\\nBarack Obama \\nJ...</td>\n",
       "      <td>[state, union, address, barack, obama, januari...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year_of_the_speech                                 text_of_the_speech  \\\n",
       "0                  1790  \\nState of the Union Address\\nGeorge Washingto...   \n",
       "1                  1791  \\nState of the Union Address\\nGeorge Washingto...   \n",
       "2                  1792  \\nState of the Union Address\\nGeorge Washingto...   \n",
       "3                  1793  \\nState of the Union Address\\nGeorge Washingto...   \n",
       "4                  1794  \\nState of the Union Address\\nGeorge Washingto...   \n",
       "..                  ...                                                ...   \n",
       "220                2008  \\nState of the Union Address\\nGeorge W. Bush\\n...   \n",
       "221                2009  \\nAddress to Joint Session of Congress \\nBarac...   \n",
       "222                2010  \\nState of the Union Address\\nBarack Obama \\nJ...   \n",
       "223                2011  \\nState of the Union Address\\nBarack Obama \\nJ...   \n",
       "224                2012  \\nState of the Union Address\\nBarack Obama \\nJ...   \n",
       "\n",
       "                                      processed_speech  \n",
       "0    [state, union, address, georg, washington, dec...  \n",
       "1    [state, union, address, georg, washington, oct...  \n",
       "2    [state, union, address, georg, washington, nov...  \n",
       "3    [state, union, address, georg, washington, dec...  \n",
       "4    [state, union, address, georg, washington, nov...  \n",
       "..                                                 ...  \n",
       "220  [state, union, address, georg, bush, januari, ...  \n",
       "221  [address, joint, session, congress, barack, ob...  \n",
       "222  [state, union, address, barack, obama, januari...  \n",
       "223  [state, union, address, barack, obama, januari...  \n",
       "224  [state, union, address, barack, obama, januari...  \n",
       "\n",
       "[225 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5564d",
   "metadata": {},
   "source": [
    "# tf-idf weighte ddocument vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baa68bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.01120939809115203)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(df[\"processed_speech\"])\n",
    "\n",
    "# 2nd iteration\n",
    "bag_of_words = [dictionary.doc2bow(item) for item in df[\"processed_speech\"]]\n",
    "\n",
    "# tf-idf scores\n",
    "tf_idf = gensim.models.TfidfModel(bag_of_words)\n",
    "tf_idf_corpus = tf_idf[bag_of_words]\n",
    "\n",
    "tf_idf_corpus[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10841ff8",
   "metadata": {},
   "source": [
    "# LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04b4702f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence scores for each number of topics: [0.541425728334292, 0.41568350780420094, 0.3914265843470043, 0.3883674108732783, 0.3796490520250867, 0.38540985279640216]\n",
      "Best number of topics: 5\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# topic range 5-30\n",
    "start_topics = 5\n",
    "end_topics = 30\n",
    "step_size = 5\n",
    "num_topics_list = list(range(start_topics, end_topics + 1, step_size))\n",
    "\n",
    "best_coherence_value = float('-inf')\n",
    "best_lsi_model = None\n",
    "\n",
    "coherence_scores = []\n",
    "\n",
    "for num_topics in num_topics_list:\n",
    "    # training\n",
    "    lsi_model = LsiModel(tf_idf_corpus, id2word=dictionary, num_topics=num_topics, chunksize=100, decay=0.5, random_seed=24)\n",
    "    \n",
    "    # coherence score\n",
    "    coherence_model = CoherenceModel(model=lsi_model, texts=df[\"processed_speech\"], dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    \n",
    "    coherence_scores.append(coherence_score)\n",
    "    \n",
    "    # save the model\n",
    "    if coherence_score > best_coherence_value:\n",
    "        best_coherence_value = coherence_score\n",
    "        best_lsi_model = lsi_model\n",
    "\n",
    "# optimal number of topics\n",
    "best_num_topics = num_topics_list[coherence_scores.index(max(coherence_scores))]\n",
    "\n",
    "print(\"Coherence scores for each number of topics:\", coherence_scores)\n",
    "print(\"Best number of topics:\", best_num_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7b317a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.209*\"tonight\" + 0.186*\"job\" + 0.157*\"child\" + 0.147*\"help\" + 0.130*\"program\" + 0.129*\"school\" + 0.123*\"budget\" + 0.120*\"america\" + 0.115*\"terrorist\" + 0.114*\"cut\" + 0.109*\"iraq\" + 0.104*\"let\" + 0.095*\"spend\" + 0.095*\"colleg\" + 0.091*\"challeng\" + 0.088*\"get\" + 0.084*\"know\" + 0.083*\"today\" + 0.082*\"tax\" + 0.081*\"deficit\"'),\n",
       " (1,\n",
       "  '-0.294*\"terrorist\" + -0.230*\"iraq\" + -0.200*\"iraqi\" + -0.132*\"terror\" + -0.122*\"tonight\" + -0.110*\"al\" + 0.103*\"program\" + -0.094*\"afghanistan\" + -0.087*\"qaeda\" + -0.085*\"regim\" + -0.073*\"saddam\" + -0.071*\"child\" + 0.071*\"econom\" + 0.071*\"agricultur\" + 0.068*\"farm\" + 0.066*\"interst\" + -0.062*\"hussein\" + 0.062*\"problem\" + 0.062*\"corpor\" + -0.060*\"america\"'),\n",
       " (2,\n",
       "  '0.319*\"terrorist\" + 0.220*\"iraq\" + 0.209*\"iraqi\" + -0.151*\"job\" + 0.145*\"terror\" + -0.126*\"colleg\" + -0.119*\"cut\" + 0.110*\"regim\" + 0.108*\"al\" + 0.103*\"afghanistan\" + -0.101*\"child\" + -0.095*\"school\" + -0.093*\"parent\" + -0.089*\"get\" + 0.084*\"enemi\" + -0.079*\"challeng\" + -0.079*\"deficit\" + -0.079*\"let\" + -0.077*\"teacher\" + 0.077*\"saddam\"'),\n",
       " (3,\n",
       "  '0.192*\"job\" + -0.170*\"program\" + -0.137*\"saddam\" + 0.131*\"qaeda\" + -0.123*\"soviet\" + 0.106*\"compani\" + 0.104*\"colleg\" + -0.103*\"budget\" + 0.101*\"iraqi\" + -0.100*\"hussein\" + 0.097*\"get\" + 0.097*\"clean\" + -0.097*\"drug\" + -0.094*\"weapon\" + -0.093*\"communist\" + 0.092*\"innov\" + 0.089*\"ca\" + 0.087*\"al\" + -0.084*\"billion\" + 0.082*\"deficit\"'),\n",
       " (4,\n",
       "  '0.208*\"child\" + -0.167*\"program\" + 0.156*\"parent\" + 0.156*\"school\" + 0.147*\"medicar\" + -0.133*\"job\" + 0.112*\"challeng\" + 0.104*\"teacher\" + 0.098*\"bipartisan\" + 0.095*\"gun\" + -0.095*\"soviet\" + -0.094*\"spend\" + 0.093*\"drug\" + 0.088*\"internet\" + 0.087*\"thank\" + 0.086*\"bosnia\" + -0.081*\"communist\" + -0.078*\"oil\" + 0.078*\"read\" + -0.077*\"get\"')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_topics = best_lsi_model.print_topics(num_topics=5, num_words=20)\n",
    "lsi_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27b91127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.209*\"tonight\" + 0.185*\"job\" + 0.157*\"child\" + 0.147*\"help\" + 0.129*\"school\" + 0.129*\"program\" + 0.123*\"budget\" + 0.121*\"america\" + 0.115*\"terrorist\" + 0.114*\"cut\" + 0.108*\"iraq\" + 0.104*\"let\" + 0.096*\"spend\" + 0.094*\"colleg\" + 0.091*\"challeng\" + 0.088*\"get\" + 0.084*\"know\" + 0.083*\"today\" + 0.082*\"tax\" + 0.081*\"deficit\"'),\n",
       " (1,\n",
       "  '-0.290*\"terrorist\" + -0.227*\"iraq\" + -0.196*\"iraqi\" + -0.131*\"terror\" + -0.121*\"tonight\" + -0.109*\"al\" + 0.107*\"program\" + -0.094*\"afghanistan\" + -0.085*\"regim\" + -0.083*\"qaeda\" + -0.077*\"saddam\" + -0.072*\"child\" + 0.071*\"econom\" + 0.070*\"agricultur\" + 0.069*\"farm\" + 0.068*\"interst\" + -0.067*\"hussein\" + 0.063*\"corpor\" + 0.062*\"problem\" + -0.059*\"america\"'),\n",
       " (2,\n",
       "  '-0.323*\"terrorist\" + -0.223*\"iraq\" + -0.212*\"iraqi\" + 0.151*\"job\" + -0.147*\"terror\" + 0.125*\"colleg\" + 0.118*\"cut\" + -0.111*\"regim\" + -0.109*\"al\" + -0.104*\"afghanistan\" + 0.100*\"child\" + 0.094*\"school\" + 0.093*\"parent\" + 0.089*\"get\" + -0.085*\"enemi\" + 0.080*\"deficit\" + 0.079*\"let\" + 0.078*\"challeng\" + 0.076*\"teacher\" + -0.076*\"qaeda\"'),\n",
       " (3,\n",
       "  '0.206*\"job\" + -0.173*\"program\" + -0.120*\"soviet\" + 0.118*\"qaeda\" + -0.111*\"saddam\" + -0.109*\"budget\" + 0.108*\"compani\" + 0.106*\"colleg\" + -0.101*\"communist\" + 0.101*\"get\" + -0.099*\"drug\" + 0.096*\"ca\" + 0.096*\"innov\" + 0.094*\"clean\" + 0.089*\"iraqi\" + -0.089*\"billion\" + 0.086*\"deficit\" + -0.084*\"weapon\" + 0.083*\"al\" + 0.081*\"iraq\"'),\n",
       " (4,\n",
       "  '0.205*\"child\" + -0.171*\"program\" + 0.155*\"school\" + 0.152*\"parent\" + 0.142*\"medicar\" + -0.123*\"job\" + 0.107*\"challeng\" + 0.106*\"teacher\" + -0.102*\"soviet\" + -0.102*\"spend\" + 0.099*\"bipartisan\" + 0.094*\"gun\" + 0.091*\"internet\" + 0.089*\"bosnia\" + -0.089*\"oil\" + 0.088*\"drug\" + 0.086*\"colleg\" + 0.085*\"thank\" + -0.078*\"communist\" + 0.078*\"grade\"'),\n",
       " (5,\n",
       "  '0.251*\"iraq\" + 0.221*\"iraqi\" + -0.216*\"terrorist\" + -0.194*\"islam\" + -0.159*\"tonight\" + -0.156*\"afghanistan\" + -0.153*\"terror\" + 0.128*\"qaeda\" + 0.128*\"al\" + -0.120*\"camp\" + -0.116*\"muslim\" + -0.099*\"regim\" + 0.095*\"extremist\" + -0.079*\"kid\" + -0.076*\"homeland\" + 0.074*\"medicar\" + -0.074*\"taliban\" + 0.073*\"saddam\" + -0.073*\"grief\" + 0.068*\"program\"'),\n",
       " (6,\n",
       "  '-0.304*\"saddam\" + -0.200*\"hussein\" + 0.159*\"terrorist\" + -0.137*\"iraq\" + -0.124*\"tonight\" + -0.112*\"gulf\" + -0.108*\"kid\" + -0.104*\"persian\" + 0.096*\"islam\" + -0.085*\"addict\" + -0.084*\"tell\" + 0.084*\"afghanistan\" + 0.083*\"terror\" + -0.082*\"drug\" + -0.077*\"homebuy\" + -0.075*\"markwel\" + 0.073*\"program\" + -0.073*\"know\" + 0.070*\"colleg\" + -0.069*\"kuwait\"'),\n",
       " (7,\n",
       "  '0.262*\"saddam\" + 0.235*\"hussein\" + -0.148*\"qaeda\" + -0.146*\"tonight\" + -0.145*\"kid\" + 0.117*\"qaida\" + 0.116*\"job\" + 0.115*\"weapon\" + -0.113*\"iraqi\" + -0.113*\"markwel\" + 0.109*\"senior\" + 0.107*\"medicar\" + 0.104*\"inspector\" + -0.095*\"extremist\" + -0.095*\"challeng\" + 0.094*\"regim\" + 0.085*\"coverag\" + 0.083*\"disarm\" + 0.082*\"mentor\" + -0.079*\"democraci\"'),\n",
       " (8,\n",
       "  '0.216*\"cut\" + -0.136*\"tonight\" + 0.130*\"coven\" + 0.124*\"welfar\" + -0.106*\"school\" + -0.105*\"clean\" + 0.103*\"bureaucraci\" + -0.101*\"energi\" + 0.099*\"iraqi\" + 0.096*\"crime\" + -0.095*\"budget\" + -0.092*\"teacher\" + 0.091*\"got\" + 0.083*\"middl\" + 0.081*\"explod\" + -0.080*\"internet\" + 0.079*\"tell\" + 0.079*\"parent\" + 0.078*\"lobbyist\" + 0.078*\"empow\"'),\n",
       " (9,\n",
       "  '-0.164*\"budget\" + -0.161*\"medicar\" + -0.160*\"steven\" + -0.146*\"josefina\" + -0.141*\"mayor\" + 0.125*\"saddam\" + -0.120*\"spend\" + 0.116*\"colleg\" + -0.096*\"joe\" + -0.091*\"health\" + -0.090*\"reform\" + -0.087*\"laura\" + 0.087*\"hussein\" + 0.086*\"challeng\" + -0.086*\"prescript\" + -0.084*\"coverag\" + 0.084*\"job\" + -0.083*\"tax\" + -0.083*\"senior\" + -0.083*\"tell\"')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use 10 since we need 10 and it has the 2nd best coherence value \n",
    "target_lsi_model = LsiModel(tf_idf_corpus, \n",
    "                            id2word=dictionary, \n",
    "                            num_topics=10, \n",
    "                            chunksize=100,  \n",
    "                            decay=0.5, \n",
    "                            random_seed=24)\n",
    "\n",
    "# topics\n",
    "lsi_topics = target_lsi_model.print_topics(num_topics=10, num_words=20)\n",
    "lsi_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b1a176c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " '-0.290*\"terrorist\" + -0.227*\"iraq\" + -0.196*\"iraqi\" + -0.131*\"terror\" + -0.121*\"tonight\" + -0.109*\"al\" + 0.107*\"program\" + -0.094*\"afghanistan\" + -0.085*\"regim\" + -0.083*\"qaeda\" + -0.077*\"saddam\" + -0.072*\"child\" + 0.071*\"econom\" + 0.070*\"agricultur\" + 0.069*\"farm\" + 0.068*\"interst\" + -0.067*\"hussein\" + 0.063*\"corpor\" + 0.062*\"problem\" + -0.059*\"america\"')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_topics[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58336b20",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a52d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
